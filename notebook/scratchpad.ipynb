{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "\n",
    "# load keys and import libraries\n",
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from thefuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# load openai key from .env and then load the OpenAI client\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0577fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# Connect to MongoDB using the connection string\n",
    "uri = \"mongodb+srv://####:####@docvectorstore.d2spgmw.mongodb.net/?retryWrites=true&w=majority&appName=DocVectorStore\"\n",
    "# Create a new client and connect to the server\n",
    "mongo_client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import tiktoken\n",
    "from typing import List, Dict\n",
    "import hashlib\n",
    "\n",
    "# Additional imports for PDF processing and tokenization\n",
    "def extract_pdf_to_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def text_to_markdown(text: str) -> str:\n",
    "    \"\"\"Convert plain text to basic markdown format\"\"\"\n",
    "    # Simple conversion - you can enhance this based on your needs\n",
    "    lines = text.split('\\n')\n",
    "    markdown_text = \"\"\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            # Basic markdown formatting\n",
    "            if len(line) < 100 and line.isupper():\n",
    "                markdown_text += f\"# {line}\\n\\n\"\n",
    "            elif line.endswith(':') and len(line) < 80:\n",
    "                markdown_text += f\"## {line}\\n\\n\"\n",
    "            else:\n",
    "                markdown_text += f\"{line}\\n\\n\"\n",
    "    \n",
    "    return markdown_text\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-4o-mini\") -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def save_markdown_to_file(markdown_text: str, pdf_path: str, output_dir: str = None) -> str:\n",
    "    \"\"\"Save markdown text to a file in the specified directory\"\"\"\n",
    "    # Get the PDF filename without extension\n",
    "    pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    # Set output directory (same as PDF if not specified)\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(pdf_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create markdown filename\n",
    "    markdown_filename = f\"{pdf_filename}.md\"\n",
    "    markdown_path = os.path.join(output_dir, markdown_filename)\n",
    "    \n",
    "    # Save markdown to file\n",
    "    with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_text)\n",
    "    \n",
    "    print(f\"Markdown saved to: {markdown_path}\")\n",
    "    return markdown_path\n",
    "\n",
    "def create_overlapping_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"Create overlapping text chunks with specified token sizes\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    chunk_id = 0\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        # Define end of chunk\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        \n",
    "        # Extract chunk tokens and decode back to text\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = encoding.decode(chunk_tokens)\n",
    "        \n",
    "        # Create chunk metadata\n",
    "        chunk_data = {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk_text,\n",
    "        }\n",
    "        \n",
    "        chunks.append(chunk_data)\n",
    "        \n",
    "        # Move start position with overlap\n",
    "        start = end - overlap\n",
    "        chunk_id += 1\n",
    "        \n",
    "        # Break if we've reached the end\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(text: str, openai_client) -> List[float]:\n",
    "    \"\"\"Generate embeddings using OpenAI API\"\"\"\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_pdf_to_mongodb(pdf_path: str, mongodb_client, openai_client, \n",
    "                          db_name: str = \"document_store\", \n",
    "                          collection_name: str = \"document_chunks\", \n",
    "                          markdown_output_dir: str = None):\n",
    "    \"\"\"Complete pipeline to process PDF and store in MongoDB\"\"\"\n",
    "    \n",
    "    # Step 1: Extract text from PDF\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    raw_text = extract_pdf_to_text(pdf_path)\n",
    "    \n",
    "    # Step 2: Convert to markdown\n",
    "    print(\"Converting to markdown...\")\n",
    "    markdown_text = text_to_markdown(raw_text)\n",
    "\n",
    "    # Step 2.5: Save markdown to file\n",
    "    print(\"Saving markdown to file...\")\n",
    "    markdown_path = save_markdown_to_file(markdown_text, pdf_path, markdown_output_dir)\n",
    "    \n",
    "    \n",
    "    # Step 3: Create chunks\n",
    "    print(\"Creating chunks...\")\n",
    "    chunks = create_overlapping_chunks(markdown_text, chunk_size=500, overlap=50)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Step 4: Generate embeddings and prepare documents for MongoDB\n",
    "    print(\"Generating embeddings...\")\n",
    "    db = mongodb_client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    documents_to_insert = []\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # Generate embedding\n",
    "        embedding = generate_embeddings(chunk[\"text\"], openai_client)\n",
    "        \n",
    "        if embedding is not None:\n",
    "            # Prepare document for MongoDB\n",
    "            document = {\n",
    "                \"source_file\": pdf_path.split('/')[-1],  # Just filename\n",
    "                \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"embedding\": embedding,\n",
    "                \"embedding_model\": \"text-embedding-3-small\",\n",
    "            }\n",
    "            \n",
    "            documents_to_insert.append(document)\n",
    "        \n",
    "        # Insert in batches of 100 to avoid memory issues\n",
    "        if len(documents_to_insert) >= 100:\n",
    "            collection.insert_many(documents_to_insert)\n",
    "            documents_to_insert = []\n",
    "            print(f\"Inserted batch, processed {i+1}/{len(chunks)} chunks\")\n",
    "    \n",
    "    # Insert remaining documents\n",
    "    if documents_to_insert:\n",
    "        collection.insert_many(documents_to_insert)\n",
    "    \n",
    "    print(f\"Successfully processed and stored {len(chunks)} chunks in MongoDB\")\n",
    "    \n",
    "    # Create index on embeddings for vector search (optional)\n",
    "    try:\n",
    "        collection.create_index([(\"embedding\", \"2dsphere\")])\n",
    "        print(\"Created index on embeddings\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Could not create embedding index: {e}\")\n",
    "\n",
    "# Usage example\n",
    "pdf_file_path = \"/Users/thyag/Desktop/Assignement/assignment-zania/dataset/raw-data/handbook.pdf\"  # Update with your PDF path\n",
    "markdown_output_directory = \"/Users/thyag/Desktop/Assignement/assignment-zania/dataset/raw-data/markdown_output\"  # Optional: specify where to save markdown\n",
    "# Note: You already have mongodb client and openai client from previous cells\n",
    "# Using the existing clients\n",
    "process_pdf_to_mongodb(\n",
    "    pdf_path=pdf_file_path,\n",
    "    mongodb_client=mongo_client,\n",
    "    openai_client=openai_client,\n",
    "    db_name=\"document_store\",\n",
    "    collection_name=\"pdf_chunks\",\n",
    "    markdown_output_dir=markdown_output_directory \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ffc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import time  # if you need delays in your workflow\n",
    "\n",
    "def vector_search_query(query_text: str, mongodb_client, openai_client,\n",
    "                       db_name: str = \"document_store\",\n",
    "                       collection_name: str = \"pdf_chunks\",\n",
    "                       index_name: str = \"vector_index\",\n",
    "                       top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Perform vector search query against MongoDB collection\"\"\"\n",
    "    print(f\"Generating embedding for query: '{query_text}'\")\n",
    "    query_embedding = generate_embeddings(query_text, openai_client)\n",
    "    if query_embedding is None:\n",
    "        print(\"Failed to generate query embedding\")\n",
    "        return []\n",
    "\n",
    "    collection = mongodb_client[db_name][collection_name]\n",
    "    print(f\"Collection: {collection.name}\")  # Debug: Print collection name\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": index_name,\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": top_k * 10,\n",
    "                \"limit\": top_k\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 1,\n",
    "                \"source_file\": 1,\n",
    "                \"chunk_id\": 1,\n",
    "                \"text\": 1,\n",
    "                \"embedding_model\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"Pipeline: {json.dumps(pipeline, indent=2)}\")  # Debug: Print pipeline\n",
    "\n",
    "    try:\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        print(f\"Found {len(results)} relevant documents\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing vector search: {e}\")\n",
    "        return []\n",
    "\n",
    "def format_search_results(results: List[Dict[str, Any]], query: str) -> None:\n",
    "    \"\"\"Format and display search results in a readable format\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SEARCH RESULTS FOR: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n--- RESULT {i} ---\")\n",
    "        print(f\"Source File: {result.get('source_file', 'Unknown')}\")\n",
    "        print(f\"Chunk ID: {result.get('chunk_id', 'Unknown')}\")\n",
    "        print(f\"Similarity Score: {result.get('score', 0):.4f}\")\n",
    "        print(f\"Embedding Model: {result.get('embedding_model', 'Unknown')}\")\n",
    "        print(\"Text Content:\")\n",
    "        print(result.get('text', ''))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "def get_collection_stats(mongodb_client, db_name: str = \"document_store\",\n",
    "                        collection_name: str = \"pdf_chunks\") -> Dict[str, Any]:\n",
    "    \"\"\"Get basic statistics about the collection\"\"\"\n",
    "    collection = mongodb_client[db_name][collection_name]\n",
    "    try:\n",
    "        total_docs = collection.count_documents({})\n",
    "        source_files = collection.distinct(\"source_file\")\n",
    "        sample_doc = collection.find_one({}, {\"_id\": 0, \"embedding\": 0})\n",
    "        print(\"Collection Statistics:\")\n",
    "        print(f\"- Total documents: {total_docs}\")\n",
    "        print(f\"- Source files: {source_files}\")\n",
    "        print(f\"- Sample document structure: {list(sample_doc.keys()) if sample_doc else 'No documents found'}\")\n",
    "        return {\n",
    "            \"total_docs\": total_docs,\n",
    "            \"source_files\": source_files,\n",
    "            \"sample_structure\": list(sample_doc.keys()) if sample_doc else []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting collection stats: {e}\")\n",
    "        return {}\n",
    "\n",
    "def search_documents_by_text(query_text: str, mongodb_client, openai_client,\n",
    "                             db_name: str = \"document_store\",\n",
    "                             collection_name: str = \"pdf_chunks\",\n",
    "                             top_k: int = 5,\n",
    "                             show_full_text: bool = True):\n",
    "    \"\"\"Complete function to search and display documents\"\"\"\n",
    "    print(f\"Searching for: '{query_text}' ‚Äî Top {top_k} results\")\n",
    "    results = vector_search_query(\n",
    "        query_text=query_text,\n",
    "        mongodb_client=mongodb_client,\n",
    "        openai_client=openai_client,\n",
    "        db_name=db_name,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    if show_full_text:\n",
    "        format_search_results(results, query_text)\n",
    "    else:\n",
    "        print(f\"\\nFound {len(results)} results:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"{i}. Score: {r.get('score', 0):.4f} | Chunk {r.get('chunk_id', 'Unknown')} | File: {r.get('source_file', 'Unknown')}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# 1) Check your collection stats first\n",
    "print(\"Getting collection statistics...\")\n",
    "stats = get_collection_stats(\n",
    "    mongodb_client=mongo_client,\n",
    "    db_name=\"document_store\",\n",
    "    collection_name=\"pdf_chunks\"\n",
    ")\n",
    "\n",
    "# 2) Wait if needed (e.g., after index creation in UI)\n",
    "# time.sleep(30)\n",
    "\n",
    "# 3) Run example searches\n",
    "example_queries = [\n",
    "    \"company\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING VECTOR SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in example_queries:\n",
    "    print(f\"\\n{'='*50}\\nQUERY: {query}\\n{'='*50}\")\n",
    "    results = search_documents_by_text(\n",
    "        query_text=query,\n",
    "        mongodb_client=mongo_client,\n",
    "        openai_client=openai_client,\n",
    "        top_k=3,\n",
    "        show_full_text=False\n",
    "    )\n",
    "    if query == \"employee benefits\":\n",
    "        print(f\"\\nShowing full text for: '{query}'\")\n",
    "        format_search_results(results, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2088b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking embedding dimension compatibility...\n",
      "Checking stored embedding dimensions...\n",
      "‚úÖ Stored embedding length: 1536\n",
      "üìÑ Sample document embedding model: text-embedding-3-small\n",
      "\n",
      "Generating test query embedding...\n",
      "‚úÖ Query embedding length: 1536\n",
      "\n",
      "==================================================\n",
      "DIMENSION COMPARISON:\n",
      "==================================================\n",
      "Stored documents: 1536 dimensions\n",
      "Query embedding:  1536 dimensions\n",
      "‚úÖ ‚úÖ DIMENSIONS MATCH! This is not the issue.\n",
      "\n",
      "==================================================\n",
      "EMBEDDING MODEL COMPARISON:\n",
      "==================================================\n",
      "Current model in generate_embeddings(): text-embedding-3-small\n",
      "Stored document models: {'text-embedding-3-small'}\n"
     ]
    }
   ],
   "source": [
    "def check_embedding_dimensions(mongodb_client, openai_client, \n",
    "                             db_name: str = \"document_store\",\n",
    "                             collection_name: str = \"pdf_chunks\"):\n",
    "    \"\"\"Check if query embeddings match stored document embeddings\"\"\"\n",
    "    \n",
    "    collection = mongodb_client[db_name][collection_name]\n",
    "    \n",
    "    # 1. Get a sample document with embedding\n",
    "    print(\"Checking stored embedding dimensions...\")\n",
    "    sample_doc = collection.find_one({\"embedding\": {\"$exists\": True}})\n",
    "    \n",
    "    if not sample_doc:\n",
    "        print(\"‚ùå No documents with embeddings found!\")\n",
    "        return False\n",
    "    \n",
    "    stored_embedding_length = len(sample_doc[\"embedding\"])\n",
    "    print(f\"‚úÖ Stored embedding length: {stored_embedding_length}\")\n",
    "    print(f\"üìÑ Sample document embedding model: {sample_doc.get('embedding_model', 'Unknown')}\")\n",
    "    \n",
    "    # 2. Generate a test query embedding\n",
    "    print(\"\\nGenerating test query embedding...\")\n",
    "    test_query = \"test query\"\n",
    "    query_embedding = generate_embeddings(test_query, openai_client)\n",
    "    \n",
    "    if query_embedding is None:\n",
    "        print(\"‚ùå Failed to generate query embedding!\")\n",
    "        return False\n",
    "    \n",
    "    query_embedding_length = len(query_embedding)\n",
    "    print(f\"‚úÖ Query embedding length: {query_embedding_length}\")\n",
    "    \n",
    "    # 3. Compare dimensions\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DIMENSION COMPARISON:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Stored documents: {stored_embedding_length} dimensions\")\n",
    "    print(f\"Query embedding:  {query_embedding_length} dimensions\")\n",
    "    \n",
    "    if stored_embedding_length == query_embedding_length:\n",
    "        print(\"‚úÖ ‚úÖ DIMENSIONS MATCH! This is not the issue.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå ‚ùå DIMENSIONS DON'T MATCH! This is likely your problem.\")\n",
    "        print(f\"Difference: {abs(stored_embedding_length - query_embedding_length)} dimensions\")\n",
    "        return False\n",
    "\n",
    "# Run the check\n",
    "print(\"Checking embedding dimension compatibility...\")\n",
    "dimensions_match = check_embedding_dimensions(\n",
    "    mongodb_client=mongo_client,\n",
    "    openai_client=openai_client\n",
    ")\n",
    "\n",
    "# Also check what embedding model you're currently using vs stored\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"EMBEDDING MODEL COMPARISON:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(\"Current model in generate_embeddings(): text-embedding-3-small\")\n",
    "\n",
    "# Get a few sample docs to see what models were used\n",
    "collection = mongo_client[\"document_store\"][\"pdf_chunks\"]\n",
    "sample_docs = list(collection.find({\"embedding_model\": {\"$exists\": True}}).limit(5))\n",
    "stored_models = [doc.get(\"embedding_model\", \"Unknown\") for doc in sample_docs]\n",
    "print(f\"Stored document models: {set(stored_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2434014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comprehensive vector search diagnosis...\n",
      "üîç DIAGNOSING VECTOR SEARCH ISSUES\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ CHECKING VECTOR INDEX...\n",
      "‚ùå Error checking search indexes: 'Collection' object is not callable. If you meant to call the 'list_search_indexes' method on a 'Collection' object it is failing because no such method exists.\n",
      "   This might indicate you're not on MongoDB Atlas or don't have search indexes\n",
      "\n",
      "‚ùå Critical issues found - vector search won't work until these are fixed!\n"
     ]
    }
   ],
   "source": [
    "def diagnose_vector_search_issues(mongodb_client, openai_client,\n",
    "                                db_name: str = \"document_store\",\n",
    "                                collection_name: str = \"pdf_chunks\",\n",
    "                                index_name: str = \"vector_index\"):\n",
    "    \"\"\"Comprehensive diagnosis of vector search issues\"\"\"\n",
    "    \n",
    "    collection = mongodb_client[db_name][collection_name]\n",
    "    \n",
    "    print(\"üîç DIAGNOSING VECTOR SEARCH ISSUES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Check if vector index exists and is active\n",
    "    print(\"\\n1Ô∏è‚É£ CHECKING VECTOR INDEX...\")\n",
    "    try:\n",
    "        indexes = list(collection.list_search_indexes())\n",
    "        print(f\"Found {len(indexes)} search indexes:\")\n",
    "        \n",
    "        vector_index_found = False\n",
    "        for idx in indexes:\n",
    "            print(f\"  - Name: {idx.get('name', 'Unknown')}\")\n",
    "            print(f\"    Status: {idx.get('status', 'Unknown')}\")\n",
    "            print(f\"    Type: {idx.get('type', 'Unknown')}\")\n",
    "            if idx.get('name') == index_name:\n",
    "                vector_index_found = True\n",
    "                if idx.get('status') != 'READY':\n",
    "                    print(f\"    ‚ö†Ô∏è  INDEX NOT READY! Status: {idx.get('status')}\")\n",
    "                else:\n",
    "                    print(f\"    ‚úÖ INDEX IS READY\")\n",
    "        \n",
    "        if not vector_index_found:\n",
    "            print(f\"‚ùå Vector index '{index_name}' NOT FOUND!\")\n",
    "            print(\"   You need to create it in MongoDB Atlas UI under Database > Search\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking search indexes: {e}\")\n",
    "        print(\"   This might indicate you're not on MongoDB Atlas or don't have search indexes\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Test the aggregation pipeline step by step\n",
    "    print(\"\\n2Ô∏è‚É£ TESTING AGGREGATION PIPELINE...\")\n",
    "    \n",
    "    # Test a simple query first\n",
    "    test_embedding = generate_embeddings(\"test\", openai_client)\n",
    "    \n",
    "    # Try the vector search without projection first\n",
    "    simple_pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": index_name,\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": test_embedding,\n",
    "                \"numCandidates\": 100,\n",
    "                \"limit\": 5\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        print(\"Testing simple vector search...\")\n",
    "        simple_results = list(collection.aggregate(simple_pipeline))\n",
    "        print(f\"‚úÖ Simple vector search returned {len(simple_results)} results\")\n",
    "        \n",
    "        if len(simple_results) > 0:\n",
    "            print(\"Sample result fields:\", list(simple_results[0].keys()))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Simple vector search failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # 3. Check if documents actually have embeddings\n",
    "    print(\"\\n3Ô∏è‚É£ CHECKING DOCUMENT STRUCTURE...\")\n",
    "    \n",
    "    total_docs = collection.count_documents({})\n",
    "    docs_with_embeddings = collection.count_documents({\"embedding\": {\"$exists\": True, \"$ne\": None}})\n",
    "    docs_with_non_empty_embeddings = collection.count_documents({\"embedding\": {\"$exists\": True, \"$type\": \"array\", \"$not\": {\"$size\": 0}}})\n",
    "    \n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Documents with embedding field: {docs_with_embeddings}\")\n",
    "    print(f\"Documents with non-empty embeddings: {docs_with_non_empty_embeddings}\")\n",
    "    \n",
    "    if docs_with_non_empty_embeddings == 0:\n",
    "        print(\"‚ùå No documents have valid embeddings!\")\n",
    "        return False\n",
    "    \n",
    "    # 4. Test with exact document text\n",
    "    print(\"\\n4Ô∏è‚É£ TESTING WITH EXACT DOCUMENT TEXT...\")\n",
    "    \n",
    "    sample_doc = collection.find_one({\"embedding\": {\"$exists\": True}})\n",
    "    if sample_doc:\n",
    "        # Try searching for exact text from a document\n",
    "        exact_text = sample_doc.get('text', '')[:50]  # First 50 chars\n",
    "        print(f\"Testing with exact text: '{exact_text}...'\")\n",
    "        \n",
    "        exact_embedding = generate_embeddings(exact_text, openai_client)\n",
    "        exact_pipeline = [\n",
    "            {\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": index_name,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"queryVector\": exact_embedding,\n",
    "                    \"numCandidates\": 10,\n",
    "                    \"limit\": 3\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"chunk_id\": 1,\n",
    "                    \"source_file\": 1,\n",
    "                    \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            exact_results = list(collection.aggregate(exact_pipeline))\n",
    "            print(f\"‚úÖ Exact text search returned {len(exact_results)} results\")\n",
    "            \n",
    "            if len(exact_results) > 0:\n",
    "                for i, result in enumerate(exact_results):\n",
    "                    print(f\"  Result {i+1}: Score {result.get('score', 0):.4f}, Chunk {result.get('chunk_id', 'Unknown')}\")\n",
    "            else:\n",
    "                print(\"‚ùå Even exact text search returned 0 results!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Exact text search failed: {e}\")\n",
    "    \n",
    "    # 5. Check MongoDB version and deployment type\n",
    "    print(\"\\n5Ô∏è‚É£ CHECKING MONGODB DEPLOYMENT...\")\n",
    "    try:\n",
    "        server_info = mongodb_client.admin.command(\"buildInfo\")\n",
    "        print(f\"MongoDB Version: {server_info.get('version', 'Unknown')}\")\n",
    "        \n",
    "        # Check if this is Atlas\n",
    "        try:\n",
    "            is_atlas = mongodb_client.admin.command(\"hello\").get(\"isAtlas\", False)\n",
    "            print(f\"Is MongoDB Atlas: {is_atlas}\")\n",
    "            if not is_atlas:\n",
    "                print(\"‚ö†Ô∏è  Vector search requires MongoDB Atlas or Enterprise\")\n",
    "        except:\n",
    "            print(\"Could not determine if this is Atlas\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not get server info: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run the comprehensive diagnosis\n",
    "print(\"Running comprehensive vector search diagnosis...\")\n",
    "diagnosis_result = diagnose_vector_search_issues(\n",
    "    mongodb_client=mongo_client,\n",
    "    openai_client=openai_client\n",
    ")\n",
    "\n",
    "if not diagnosis_result:\n",
    "    print(\"\\n‚ùå Critical issues found - vector search won't work until these are fixed!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No obvious issues found - the problem might be more subtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8af1533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running legacy vector search diagnosis...\n",
      "üîç DIAGNOSING VECTOR SEARCH ISSUES (Legacy Mode)\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ VECTOR INDEX STATUS...\n",
      "‚ö†Ô∏è  Cannot check index status with this pymongo version\n",
      "   Please verify in MongoDB Atlas UI that you have created a vector search index named 'vector_index'\n",
      "   Go to: Database ‚Üí Search ‚Üí Create Search Index ‚Üí JSON Editor\n",
      "   Use this configuration:\n",
      "\n",
      "    {\n",
      "      \"fields\": [\n",
      "        {\n",
      "          \"type\": \"vector\",\n",
      "          \"path\": \"embedding\",\n",
      "          \"numDimensions\": 1536,\n",
      "          \"similarity\": \"cosine\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "    \n",
      "\n",
      "2Ô∏è‚É£ CHECKING DOCUMENT STRUCTURE...\n",
      "Total documents: 62\n",
      "Documents with embedding field: 62\n",
      "‚úÖ Documents have embeddings\n",
      "\n",
      "3Ô∏è‚É£ TESTING EMBEDDING GENERATION...\n",
      "‚úÖ Can generate embeddings (length: 1536)\n",
      "\n",
      "4Ô∏è‚É£ CHECKING EMBEDDING DIMENSIONS...\n",
      "Stored embedding length: 1536\n",
      "Query embedding length: 1536\n",
      "‚úÖ Embedding dimensions match\n",
      "\n",
      "5Ô∏è‚É£ TESTING VECTOR SEARCH...\n",
      "‚úÖ Vector search returned 5 results\n",
      "Sample results:\n",
      "  Result 1: Score 0.5891\n",
      "    Text preview:  as:\n",
      "\n",
      "Email systems and accounts.\n",
      "\n",
      "Internet and in...\n",
      "  Result 2: Score 0.5886\n",
      "    Text preview:  race (including, but not limited to, hair texture...\n",
      "\n",
      "6Ô∏è‚É£ CHECKING MONGODB DEPLOYMENT...\n",
      "MongoDB Version: 8.0.10\n",
      "Is MongoDB Atlas: False\n",
      "‚ö†Ô∏è  Vector search requires MongoDB Atlas\n"
     ]
    }
   ],
   "source": [
    "def diagnose_vector_search_issues_legacy(mongodb_client, openai_client,\n",
    "                                       db_name: str = \"document_store\",\n",
    "                                       collection_name: str = \"pdf_chunks\",\n",
    "                                       index_name: str = \"vector_store\"):\n",
    "    \"\"\"Comprehensive diagnosis of vector search issues for older pymongo versions\"\"\"\n",
    "    \n",
    "    collection = mongodb_client[db_name][collection_name]\n",
    "    \n",
    "    print(\"üîç DIAGNOSING VECTOR SEARCH ISSUES (Legacy Mode)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Skip index checking for now (requires newer pymongo)\n",
    "    print(\"\\n1Ô∏è‚É£ VECTOR INDEX STATUS...\")\n",
    "    print(\"‚ö†Ô∏è  Cannot check index status with this pymongo version\")\n",
    "    print(\"   Please verify in MongoDB Atlas UI that you have created a vector search index named 'vector_index'\")\n",
    "    print(\"   Go to: Database ‚Üí Search ‚Üí Create Search Index ‚Üí JSON Editor\")\n",
    "    print(\"   Use this configuration:\")\n",
    "    print(\"\"\"\n",
    "    {\n",
    "      \"fields\": [\n",
    "        {\n",
    "          \"type\": \"vector\",\n",
    "          \"path\": \"embedding\",\n",
    "          \"numDimensions\": 1536,\n",
    "          \"similarity\": \"cosine\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    # 2. Check if documents actually have embeddings\n",
    "    print(\"\\n2Ô∏è‚É£ CHECKING DOCUMENT STRUCTURE...\")\n",
    "    \n",
    "    total_docs = collection.count_documents({})\n",
    "    docs_with_embeddings = collection.count_documents({\"embedding\": {\"$exists\": True, \"$ne\": None}})\n",
    "    \n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Documents with embedding field: {docs_with_embeddings}\")\n",
    "    \n",
    "    if docs_with_embeddings == 0:\n",
    "        print(\"‚ùå No documents have valid embeddings!\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ Documents have embeddings\")\n",
    "    \n",
    "    # 3. Test embedding generation\n",
    "    print(\"\\n3Ô∏è‚É£ TESTING EMBEDDING GENERATION...\")\n",
    "    test_embedding = generate_embeddings(\"test query\", openai_client)\n",
    "    if test_embedding:\n",
    "        print(f\"‚úÖ Can generate embeddings (length: {len(test_embedding)})\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot generate embeddings\")\n",
    "        return False\n",
    "    \n",
    "    # 4. Check embedding dimensions\n",
    "    print(\"\\n4Ô∏è‚É£ CHECKING EMBEDDING DIMENSIONS...\")\n",
    "    sample_doc = collection.find_one({\"embedding\": {\"$exists\": True}})\n",
    "    if sample_doc:\n",
    "        stored_length = len(sample_doc[\"embedding\"])\n",
    "        query_length = len(test_embedding)\n",
    "        print(f\"Stored embedding length: {stored_length}\")\n",
    "        print(f\"Query embedding length: {query_length}\")\n",
    "        \n",
    "        if stored_length == query_length:\n",
    "            print(\"‚úÖ Embedding dimensions match\")\n",
    "        else:\n",
    "            print(\"‚ùå Embedding dimensions don't match!\")\n",
    "            return False\n",
    "    \n",
    "    # 5. Test basic vector search\n",
    "    print(\"\\n5Ô∏è‚É£ TESTING VECTOR SEARCH...\")\n",
    "    try:\n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": index_name,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"queryVector\": test_embedding,\n",
    "                    \"numCandidates\": 100,\n",
    "                    \"limit\": 5\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"chunk_id\": 1,\n",
    "                    \"source_file\": 1,\n",
    "                    \"text\": {\"$substr\": [\"$text\", 0, 100]},\n",
    "                    \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        print(f\"‚úÖ Vector search returned {len(results)} results\")\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            print(\"Sample results:\")\n",
    "            for i, result in enumerate(results[:2]):\n",
    "                print(f\"  Result {i+1}: Score {result.get('score', 0):.4f}\")\n",
    "                print(f\"    Text preview: {result.get('text', 'No text')[:50]}...\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Vector search returned 0 results\")\n",
    "            print(\"   This likely means:\")\n",
    "            print(\"   - Vector index doesn't exist or isn't named 'vector_index'\")\n",
    "            print(\"   - Index is still building\")\n",
    "            print(\"   - Index configuration is incorrect\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Vector search failed: {e}\")\n",
    "        print(\"   This usually means:\")\n",
    "        print(\"   - No vector search index exists\")\n",
    "        print(\"   - You're not on MongoDB Atlas\")\n",
    "        print(\"   - Index name is incorrect\")\n",
    "        return False\n",
    "    \n",
    "    # 6. Check MongoDB version and deployment type\n",
    "    print(\"\\n6Ô∏è‚É£ CHECKING MONGODB DEPLOYMENT...\")\n",
    "    try:\n",
    "        server_info = mongodb_client.admin.command(\"buildInfo\")\n",
    "        print(f\"MongoDB Version: {server_info.get('version', 'Unknown')}\")\n",
    "        \n",
    "        # Check if this is Atlas\n",
    "        try:\n",
    "            is_atlas = mongodb_client.admin.command(\"hello\").get(\"isAtlas\", False)\n",
    "            print(f\"Is MongoDB Atlas: {is_atlas}\")\n",
    "            if not is_atlas:\n",
    "                print(\"‚ö†Ô∏è  Vector search requires MongoDB Atlas\")\n",
    "        except:\n",
    "            print(\"Could not determine if this is Atlas\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not get server info: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run the legacy diagnosis\n",
    "print(\"Running legacy vector search diagnosis...\")\n",
    "diagnosis_result = diagnose_vector_search_issues_legacy(\n",
    "    mongodb_client=mongo_client,\n",
    "    openai_client=openai_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28bc46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TESTING WITH CORRECT INDEX NAME: 'vector_store'\n",
      "============================================================\n",
      "‚úÖ Vector search with 'vector_store' index returned 5 results\n",
      "‚úÖ SUCCESS! Vector search is working!\n",
      "Sample results:\n",
      "  Result 1: Score 0.5892\n",
      "    Chunk 25\n",
      "    Text preview:  as:\n",
      "\n",
      "Email systems and accounts.\n",
      "\n",
      "Internet and in...\n",
      "  Result 2: Score 0.5886\n",
      "    Chunk 42\n",
      "    Text preview:  race (including, but not limited to, hair texture...\n",
      "\n",
      "üéâ PROBLEM SOLVED! The issue was the incorrect index name.\n",
      "   Your index is named 'vector_store', not 'vector_index'\n"
     ]
    }
   ],
   "source": [
    "# Test the diagnosis with correct index name\n",
    "def test_vector_search_with_correct_index(mongodb_client, openai_client):\n",
    "    \"\"\"Test vector search with the correct index name\"\"\"\n",
    "    \n",
    "    collection = mongodb_client[\"document_store\"][\"pdf_chunks\"]\n",
    "    \n",
    "    print(\"üîç TESTING WITH CORRECT INDEX NAME: 'vector_store'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Generate test embedding\n",
    "    test_embedding = generate_embeddings(\"test query\", openai_client)\n",
    "    if not test_embedding:\n",
    "        print(\"‚ùå Cannot generate embeddings\")\n",
    "        return False\n",
    "    \n",
    "    # Test vector search with correct index name\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_store\",  # Using correct index name\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": test_embedding,\n",
    "                \"numCandidates\": 100,\n",
    "                \"limit\": 5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"chunk_id\": 1,\n",
    "                \"source_file\": 1,\n",
    "                \"text\": {\"$substr\": [\"$text\", 0, 100]},\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        print(f\"‚úÖ Vector search with 'vector_store' index returned {len(results)} results\")\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            print(\"‚úÖ SUCCESS! Vector search is working!\")\n",
    "            print(\"Sample results:\")\n",
    "            for i, result in enumerate(results[:2]):\n",
    "                print(f\"  Result {i+1}: Score {result.get('score', 0):.4f}\")\n",
    "                print(f\"    Chunk {result.get('chunk_id', 'Unknown')}\")\n",
    "                print(f\"    Text preview: {result.get('text', 'No text')[:50]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Still getting 0 results with correct index name\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Vector search failed even with correct index name: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "success = test_vector_search_with_correct_index(mongo_client, openai_client)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ PROBLEM SOLVED! The issue was the incorrect index name.\")\n",
    "    print(\"   Your index is named 'vector_store', not 'vector_index'\")\n",
    "else:\n",
    "    print(\"\\nüòï Still having issues even with correct index name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1923870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING VECTOR SEARCH\n",
      "================================================================================\n",
      "\n",
      "QUERY: company\n",
      "----------------------------------------\n",
      "Searching for: 'company' ‚Äî Top 3 results\n",
      "Found 3 results:\n",
      "1. Score: 0.6556 | Chunk 23 | File: handbook.pdf\n",
      "2. Score: 0.6492 | Chunk 3 | File: handbook.pdf\n",
      "3. Score: 0.6425 | Chunk 18 | File: handbook.pdf\n",
      "\n",
      "Detailed results for 'company':\n",
      "\n",
      "================================================================================\n",
      "SEARCH RESULTS FOR: 'company'\n",
      "================================================================================\n",
      "\n",
      "--- RESULT 1 ---\n",
      "Source File: handbook.pdf\n",
      "Chunk ID: 23\n",
      "Similarity Score: 0.6556\n",
      "Embedding Model: text-embedding-3-small\n",
      "Text Content:\n",
      " communication.\n",
      "\n",
      "Company principles, guidelines, and policies apply to online activities just as they apply to other areas of work. Ultimately,\n",
      "\n",
      "you are solely responsible for what you communicate in social media. You may be personally responsible for any litigation\n",
      "\n",
      "that may arise should you make unlawful defamatory, slanderous, or libelous statements against any customer, manager,\n",
      "\n",
      "owner, or employees of the Company.\n",
      "\n",
      "Know and Follow the Rules\n",
      "\n",
      "Ensure your postings are consistent with these guidelines. Postings that include unlawful discriminatory remarks,\n",
      "\n",
      "harassment, and threats of violence or other unlawful conduct will not be tolerated and may subject you to disciplinary action\n",
      "\n",
      "up to and including termination.\n",
      "\n",
      "Be Respectful\n",
      "\n",
      "The Company cannot force or mandate respectful and courteous activity by employees on social media during nonworking\n",
      "\n",
      "time. If you decide to post complaints or criticism, avoid using statements, photographs, video, or audio that reasonably\n",
      "\n",
      "could be viewed as unlawful, slanderous, threatening, or that might constitute unlawful harassment. Examples of such\n",
      "\n",
      "conduct might include defamatory or slanderous posts meant to harm someone's reputation or posts that could contribute to\n",
      "\n",
      "a hostile work environment on the basis of race, sex, disability, age, national origin, religion, veteran status, or any other\n",
      "\n",
      "status or class protected by law or Company policy. Your personal posts and social media activity should not reflect upon or\n",
      "\n",
      "refer to the Company.\n",
      "\n",
      "Maintain Accuracy and Confidentiality\n",
      "\n",
      "## When posting information:\n",
      "\n",
      "Maintain the confidentiality of trade secrets, intellectual property, and confidential commercially-sensitive information\n",
      "\n",
      "(i.e. financial or sales records/reports, marketing or business strategies/plans, product development, customer lists,\n",
      "\n",
      "patents, trademarks, etc.) related to the Company.\n",
      "\n",
      "Do not create a link from your personal blog, website, or other social networking site to a Company website that\n",
      "\n",
      "identifies you as speaking on behalf of the Company.\n",
      "\n",
      "Never represent yourself as a spokesperson for the Company. If the Company is a subject of the content you are\n",
      "\n",
      "creating, do not represent yourself as speaking on behalf of the Company. Make it clear in your social media activity\n",
      "\n",
      "that you are speaking on your own behalf.\n",
      "\n",
      "Respect copyright, trademark, third-party rights, and similar laws and use such protected information in compliance\n",
      "\n",
      "with applicable legal standards.\n",
      "\n",
      "Using Social Media at Work\n",
      "\n",
      "Do not use social media while on your work time, unless it is work related as authorized by your manager or consistent with\n",
      "\n",
      "policies that cover equipment owned by the Company.\n",
      "\n",
      "Media Contacts\n",
      "\n",
      "If you are not\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- RESULT 2 ---\n",
      "Source File: handbook.pdf\n",
      "Chunk ID: 3\n",
      "Similarity Score: 0.6492\n",
      "Embedding Model: text-embedding-3-small\n",
      "Text Content:\n",
      " modify, or supplement the provisions of this handbook at any\n",
      "\n",
      "time. Neither this handbook nor any other communication by a management representative or other, whether oral or written,\n",
      "\n",
      "is intended in any way to create a contract of employment. Please understand that no employee handbook can address\n",
      "\n",
      "every situation in the work place.\n",
      "\n",
      "If you have questions about your employment or any provisions in this handbook, contact People Operations.\n",
      "\n",
      "We wish you success in your employment here at Zania, Inc.!\n",
      "\n",
      "All the best,\n",
      "\n",
      "Shruti Gupta, CEO\n",
      "\n",
      "Zania, Inc.\n",
      "\n",
      "1.2\n",
      "\n",
      "At-Will Employment\n",
      "\n",
      "Your employment with Zania, Inc. is on an \"at-will\" basis. This means your employment may be terminated at any time, with\n",
      "\n",
      "or without notice and with or without cause. Likewise, we respect your right to leave the Company at any time, with or\n",
      "\n",
      "without notice and with or without cause.\n",
      "\n",
      "Nothing in this handbook or any other Company document should be understood as creating a contract, guaranteed or\n",
      "\n",
      "continued employment, a right to termination only \"for cause,\" or any other guarantee of continued benefits or employment.\n",
      "\n",
      "Only the CEO has the authority to make promises or negotiate with regard to guaranteed or continued employment, and any\n",
      "\n",
      "such promises are only effective if placed in writing and signed by the CEO.\n",
      "\n",
      "If a written contract between you and the Company is inconsistent with this handbook, the written contract is controlling.\n",
      "\n",
      "Nothing in this handbook will be interpreted, applied, or enforced to interfere with, restrain, or coerce employees in the\n",
      "\n",
      "exercise of their rights under Section 7 of the National Labor Relations Act.\n",
      "\n",
      "4\n",
      "\n",
      "This policy may not be appropriate in its entirety for employees working in Montana.\n",
      "\n",
      "2.0\n",
      "\n",
      "Introductory Language and Policies\n",
      "\n",
      "2.1\n",
      "\n",
      "About the Company\n",
      "\n",
      "[[Add your about the company statement here.]]\n",
      "\n",
      "2.2\n",
      "\n",
      "Company Facilities\n",
      "\n",
      "[[Insert information about your company facilities here.]]\n",
      "\n",
      "2.3\n",
      "\n",
      "Ethics Code\n",
      "\n",
      "Zania, Inc. will conduct business honestly and ethically wherever operations are maintained. We strive to improve the quality\n",
      "\n",
      "of our services, products, and operations and will maintain a reputation for honesty, fairness, respect, responsibility,\n",
      "\n",
      "integrity, trust, and sound business judgment. Our managers and employees are expected to adhere to high standards of\n",
      "\n",
      "business and personal integrity as a representation of our business practices, at all times consistent with their duty of loyalty\n",
      "\n",
      "to the Company.\n",
      "\n",
      "We expect that officers, directors, and employees will not\n",
      "------------------------------------------------------------\n",
      "\n",
      "QUERY: employee benefits\n",
      "----------------------------------------\n",
      "Searching for: 'employee benefits' ‚Äî Top 3 results\n",
      "Found 3 results:\n",
      "1. Score: 0.7415 | Chunk 8 | File: handbook.pdf\n",
      "2. Score: 0.7275 | Chunk 1 | File: handbook.pdf\n",
      "3. Score: 0.7195 | Chunk 15 | File: handbook.pdf\n",
      "\n",
      "QUERY: vacation policy\n",
      "----------------------------------------\n",
      "Searching for: 'vacation policy' ‚Äî Top 3 results\n",
      "Found 3 results:\n",
      "1. Score: 0.8025 | Chunk 33 | File: handbook.pdf\n",
      "2. Score: 0.7709 | Chunk 32 | File: handbook.pdf\n",
      "3. Score: 0.7638 | Chunk 31 | File: handbook.pdf\n",
      "\n",
      "QUERY: salary information\n",
      "----------------------------------------\n",
      "Searching for: 'salary information' ‚Äî Top 3 results\n",
      "Found 3 results:\n",
      "1. Score: 0.6912 | Chunk 8 | File: handbook.pdf\n",
      "2. Score: 0.6799 | Chunk 51 | File: handbook.pdf\n",
      "3. Score: 0.6592 | Chunk 9 | File: handbook.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "def vector_search_query(query_text: str, mongodb_client, openai_client,\n",
    "                       db_name: str = \"document_store\",\n",
    "                       collection_name: str = \"pdf_chunks\",\n",
    "                       index_name: str = \"vector_store\",  # Changed to correct index name\n",
    "                       top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Perform vector search query against MongoDB collection\"\"\"\n",
    "    query_embedding = generate_embeddings(query_text, openai_client)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "\n",
    "    collection = mongodb_client[db_name][collection_name]\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": index_name,\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": top_k * 10,\n",
    "                \"limit\": top_k\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 1,\n",
    "                \"source_file\": 1,\n",
    "                \"chunk_id\": 1,\n",
    "                \"text\": 1,\n",
    "                \"embedding_model\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing vector search: {e}\")\n",
    "        return []\n",
    "\n",
    "def format_search_results(results: List[Dict[str, Any]], query: str) -> None:\n",
    "    \"\"\"Format and display search results in a readable format\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SEARCH RESULTS FOR: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n--- RESULT {i} ---\")\n",
    "        print(f\"Source File: {result.get('source_file', 'Unknown')}\")\n",
    "        print(f\"Chunk ID: {result.get('chunk_id', 'Unknown')}\")\n",
    "        print(f\"Similarity Score: {result.get('score', 0):.4f}\")\n",
    "        print(f\"Embedding Model: {result.get('embedding_model', 'Unknown')}\")\n",
    "        print(\"Text Content:\")\n",
    "        print(result.get('text', ''))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "def search_documents_by_text(query_text: str, mongodb_client, openai_client,\n",
    "                             db_name: str = \"document_store\",\n",
    "                             collection_name: str = \"pdf_chunks\",\n",
    "                             top_k: int = 5,\n",
    "                             show_full_text: bool = True):\n",
    "    \"\"\"Complete function to search and display documents\"\"\"\n",
    "    print(f\"Searching for: '{query_text}' ‚Äî Top {top_k} results\")\n",
    "    results = vector_search_query(\n",
    "        query_text=query_text,\n",
    "        mongodb_client=mongodb_client,\n",
    "        openai_client=openai_client,\n",
    "        db_name=db_name,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    if show_full_text:\n",
    "        format_search_results(results, query_text)\n",
    "    else:\n",
    "        print(f\"Found {len(results)} results:\")\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"{i}. Score: {r.get('score', 0):.4f} | Chunk {r.get('chunk_id', 'Unknown')} | File: {r.get('source_file', 'Unknown')}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test the vector search with various queries\n",
    "test_queries = [\n",
    "    \"company\",\n",
    "    \"employee benefits\", \n",
    "    \"vacation policy\",\n",
    "    \"salary information\"\n",
    "]\n",
    "\n",
    "print(\"TESTING VECTOR SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQUERY: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    results = search_documents_by_text(\n",
    "        query_text=query,\n",
    "        mongodb_client=mongo_client,\n",
    "        openai_client=openai_client,\n",
    "        top_k=3,\n",
    "        show_full_text=False\n",
    "    )\n",
    "    \n",
    "    # Show full text for the first query as example\n",
    "    if query == test_queries[0] and results:\n",
    "        print(f\"\\nDetailed results for '{query}':\")\n",
    "        format_search_results(results[:2], query)  # Show top 2 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f72962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TESTING QUESTION ANSWERING WITH RAG\n",
      "================================================================================\n",
      "ü§ñ STARTING BATCH QUESTION ANSWERING\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 1/4\n",
      "\n",
      "üîç QUESTION: What is the name of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "The name of the company is Zania, Inc.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. handbook.pdf (Chunk 46) - Similarity: 0.6721\n",
      "2. handbook.pdf (Chunk 17) - Similarity: 0.6606\n",
      "3. handbook.pdf (Chunk 3) - Similarity: 0.6577\n",
      "\n",
      "üìã DETAILED CONTEXT FOR FIRST QUESTION:\n",
      "\n",
      "================================================================================\n",
      "SEARCH RESULTS FOR: 'What is the name of the company?'\n",
      "================================================================================\n",
      "\n",
      "--- RESULT 1 ---\n",
      "Source File: handbook.pdf\n",
      "Chunk ID: 46\n",
      "Similarity Score: 0.6721\n",
      "Embedding Model: text-embedding-3-small\n",
      "Text Content:\n",
      " or the nearest EEOC or CRD office.\n",
      "\n",
      "Filing of Complaints Outside Company\n",
      "\n",
      "You may file formal complaints of discrimination, harassment, or retaliation with the agencies listed below. Contact these\n",
      "\n",
      "33\n",
      "\n",
      "agencies directly for more information about filing processes.\n",
      "\n",
      "California Civil Rights Department\n",
      "\n",
      "2218 Kausen Drive, Suite 100\n",
      "\n",
      "Elk Grove, CA 95758\n",
      "\n",
      "Voice: 800-884-1684\n",
      "\n",
      "# TTY: 800-700-2320\n",
      "\n",
      "California Relay Service: 711\n",
      "\n",
      "Email: contact.center@dfeh.ca.gov\n",
      "\n",
      "## Main website:\n",
      "\n",
      "https://www.calcivilrights.ca.gov\n",
      "\n",
      "## Online sexual harassment training courses:\n",
      "\n",
      "https://www.calcivilrights.ca.gov/shpt/\n",
      "\n",
      "U.S. Equal Employment Opportunity Commission\n",
      "\n",
      "450 Golden Gate Avenue 5 West\n",
      "\n",
      "P.O. Box 36025\n",
      "\n",
      "San Francisco, CA 94102-3661\n",
      "\n",
      "Phone: 800-669-4000\n",
      "\n",
      "Fax: 415-522-3415\n",
      "\n",
      "# TTY: 800-669-6820\n",
      "\n",
      "ASL Video Phone: 844-234-5122\n",
      "\n",
      "https://www.eeoc.gov/field-office/sanfrancisco/location\n",
      "\n",
      "Religious Accommodation\n",
      "\n",
      "Zania, Inc. is dedicated to treating its employees equally and with respect and recognizes the diversity of their religious\n",
      "\n",
      "beliefs. All employees, unpaid interns, and volunteers may request an accommodation when their religious beliefs cause a\n",
      "\n",
      "deviation from the Company dress or grooming code, or the individual's schedule, basic job duties, or other aspects of\n",
      "\n",
      "employment. The Company will consider the request, but reserves the right to offer its own accommodation to the extent\n",
      "\n",
      "permitted by law. Some, but not all, of the factors that the Company will consider are cost, the effect that an accommodation\n",
      "\n",
      "will have on current established policies, and the burden on operations ‚Äî including other employees ‚Äî when determining a\n",
      "\n",
      "reasonable accommodation. At no time will the Company question the validity of a person's belief.\n",
      "\n",
      "If you require a religious accommodation, speak with your Manager [[or appropriate department]].\n",
      "\n",
      "Wage and Hour Policies\n",
      "\n",
      "Accommodations for Nursing Mothers\n",
      "\n",
      "Zania, Inc. is required by law to provide requesting employees who are nursing mothers with certain accommodations to\n",
      "\n",
      "## express milk. Accordingly, the Company will provide nursing mothers with:\n",
      "\n",
      "Reasonable break time to express milk for their infant child each time the mother has the need to express milk; and\n",
      "\n",
      "A private room or other location\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- RESULT 2 ---\n",
      "Source File: handbook.pdf\n",
      "Chunk ID: 17\n",
      "Similarity Score: 0.6606\n",
      "Embedding Model: text-embedding-3-small\n",
      "Text Content:\n",
      ", contractors, suppliers, or vendors.\n",
      "\n",
      "Refusal or failure to follow directions or to perform a requested or required job task.\n",
      "\n",
      "Refusal or failure to follow safety rules and procedures.\n",
      "\n",
      "Excessive tardiness or absences.\n",
      "\n",
      "Smoking in nondesignated areas.\n",
      "\n",
      "Working unauthorized overtime.\n",
      "\n",
      "Solicitation of fellow employees on Company premises during working hours.\n",
      "\n",
      "Failure to dress according to Company policy.\n",
      "\n",
      "Use of obscene or harassing (as defined by our EEO policy) language in the workplace.\n",
      "\n",
      "Engaging in outside employment that interferes with your ability to perform your job at this Company.\n",
      "\n",
      "Gambling on Company premises.\n",
      "\n",
      "Lending keys or keycards to Company property to unauthorized persons.\n",
      "\n",
      "Nothing in this policy is intended to limit your rights under the National Labor Relations Act, or to modify the at-will\n",
      "\n",
      "employment status where at-will is not prohibited by state law.\n",
      "\n",
      "5.12\n",
      "\n",
      "Transfers\n",
      "\n",
      "Zania, Inc. may transfer your employment from one position to another with or without notice, as required by production or\n",
      "\n",
      "service needs, or upon request by you and with management approval. Transfers in excess of 90 days may be considered\n",
      "\n",
      "final and your paycheck may be increased or decreased consistent with the pay scale for your new position.\n",
      "\n",
      "5.13\n",
      "\n",
      "Workforce Reductions (Layoffs)\n",
      "\n",
      "If necessary based upon business needs, Zania, Inc. management may decide to implement a reduction in force (RIF). We\n",
      "\n",
      "acknowledge that RIFs can be a trying experience for all involved, and the Company will make its best effort to make sound\n",
      "\n",
      "business decisions while acknowledging the needs of its workforce.\n",
      "\n",
      "6.0\n",
      "\n",
      "General Policies\n",
      "\n",
      "6.1\n",
      "\n",
      "Computer Security and Copying of Software\n",
      "\n",
      "14\n",
      "\n",
      "Software programs purchased and provided by Zania, Inc. are to be used only for creating, researching, and processing\n",
      "\n",
      "materials for Company use. By using Company hardware, software, and networking systems you assume personal\n",
      "\n",
      "responsibility for their use and agree to comply with this policy and other applicable Company policies, as well as city, state,\n",
      "\n",
      "and federal laws and regulations.\n",
      "\n",
      "All software acquired for or on behalf of the Company, or developed by Company employees or contract personnel on behalf\n",
      "\n",
      "of the Company, is and will be deemed Company property. It is the policy of the Company to respect all computer software\n",
      "\n",
      "rights and to adhere to the terms of all software licenses to which the Company is a party. The [[Director of Information\n",
      "\n",
      "Systems]] is responsible for enforcing these guidelines.\n",
      "\n",
      "You\n",
      "------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 2/4\n",
      "\n",
      "üîç QUESTION: Who is the CEO of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "The CEO of the company is Shruti Gupta.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. handbook.pdf (Chunk 3) - Similarity: 0.6806\n",
      "2. handbook.pdf (Chunk 60) - Similarity: 0.6636\n",
      "3. handbook.pdf (Chunk 23) - Similarity: 0.6582\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 3/4\n",
      "\n",
      "üîç QUESTION: What is their vacation policy?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "Zania, Inc. provides paid vacation to all full-time regular employees, who are eligible to begin using vacation immediately upon hire or after completing their introductory period. Employees must request vacation from their Manager in advance, with the Company generally granting requests based on business needs. Vacation must be taken in increments of at least a specified number of hours or days.\n",
      "\n",
      "Unused vacation may be carried over to the following year or forfeited, depending on the employer's choice. Upon separation of employment, employees will either forfeit any earned but unused vacation time or be paid for it, depending on state law.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. handbook.pdf (Chunk 33) - Similarity: 0.7659\n",
      "2. handbook.pdf (Chunk 32) - Similarity: 0.7634\n",
      "3. handbook.pdf (Chunk 31) - Similarity: 0.7538\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 4/4\n",
      "\n",
      "üîç QUESTION: What is the termination policy?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "The termination policy includes the following key points:\n",
      "\n",
      "1. **Disciplinary Process**: Violations of company policies may result in disciplinary action, including demotion, transfer, leave without pay, or termination. The company encourages progressive discipline but is not required to follow it.\n",
      "\n",
      "2. **Criminal Activity**: Involvement in criminal activity may lead to disciplinary action, including termination.\n",
      "\n",
      "3. **Attendance**: Failure to report to work as scheduled may result in disciplinary action, up to termination, for violating attendance policies or job abandonment.\n",
      "\n",
      "4. **Final Pay**: Separated employees will be paid in accordance with applicable laws.\n",
      "\n",
      "5. **Return of Property**: Employees must return all company property at the time of separation, and failure to do so may result in deductions from the final paycheck.\n",
      "\n",
      "6. **Notice**: Employees are requested to provide a minimum of two weeks' notice of resignation (four weeks for managers). Providing less notice may affect rehire eligibility.\n",
      "\n",
      "7. **Pay in Lieu of Notice**: The company reserves the right to provide pay in lieu of notice if job or business needs warrant it. \n",
      "\n",
      "Overall, the policy emphasizes professionalism in handling resignations and outlines the responsibilities of employees upon termination.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. handbook.pdf (Chunk 15) - Similarity: 0.7276\n",
      "2. handbook.pdf (Chunk 12) - Similarity: 0.7107\n",
      "3. handbook.pdf (Chunk 26) - Similarity: 0.7008\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY OF ALL QUESTIONS AND ANSWERS\n",
      "================================================================================\n",
      "\n",
      "1. Q: What is the name of the company?\n",
      "   A: The name of the company is Zania, Inc.\n",
      "   Sources: 5 relevant chunks found\n",
      "\n",
      "2. Q: Who is the CEO of the company?\n",
      "   A: The CEO of the company is Shruti Gupta.\n",
      "   Sources: 5 relevant chunks found\n",
      "\n",
      "3. Q: What is their vacation policy?\n",
      "   A: Zania, Inc. provides paid vacation to all full-time regular employees, who are eligible to begin usi...\n",
      "   Sources: 5 relevant chunks found\n",
      "\n",
      "4. Q: What is the termination policy?\n",
      "   A: The termination policy includes the following key points:\n",
      "\n",
      "1. **Disciplinary Process**: Violations o...\n",
      "   Sources: 5 relevant chunks found\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "def vector_search_query(query_text: str, mongodb_client, openai_client,\n",
    "                       db_name: str = \"document_store\",\n",
    "                       collection_name: str = \"pdf_chunks\",\n",
    "                       index_name: str = \"vector_store\",\n",
    "                       top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Perform vector search query against MongoDB collection\"\"\"\n",
    "    query_embedding = generate_embeddings(query_text, openai_client)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "\n",
    "    collection = mongodb_client[db_name][collection_name]\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": index_name,\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": top_k * 10,\n",
    "                \"limit\": top_k\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 1,\n",
    "                \"source_file\": 1,\n",
    "                \"chunk_id\": 1,\n",
    "                \"text\": 1,\n",
    "                \"embedding_model\": 1,\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing vector search: {e}\")\n",
    "        return []\n",
    "\n",
    "def format_search_results(results: List[Dict[str, Any]], query: str) -> None:\n",
    "    \"\"\"Format and display search results in a readable format\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SEARCH RESULTS FOR: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n--- RESULT {i} ---\")\n",
    "        print(f\"Source File: {result.get('source_file', 'Unknown')}\")\n",
    "        print(f\"Chunk ID: {result.get('chunk_id', 'Unknown')}\")\n",
    "        print(f\"Similarity Score: {result.get('score', 0):.4f}\")\n",
    "        print(f\"Embedding Model: {result.get('embedding_model', 'Unknown')}\")\n",
    "        print(\"Text Content:\")\n",
    "        print(result.get('text', ''))\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "def answer_question_with_context(question: str, context_chunks: List[Dict[str, Any]], openai_client) -> str:\n",
    "    \"\"\"Use GPT-4o-mini to answer questions based on retrieved context\"\"\"\n",
    "    if not context_chunks:\n",
    "        return \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    # Combine context from all chunks\n",
    "    context_text = \"\\n\\n\".join([chunk.get('text', '') for chunk in context_chunks])\n",
    "    \n",
    "    # Create prompt for GPT-4o-mini\n",
    "    prompt = f\"\"\"Based on the following context from the document, please answer the question. If the answer is not clearly available in the context, please say so.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided document context. Be concise and accurate.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer due to an error.\"\n",
    "\n",
    "def search_and_answer_question(question: str, mongodb_client, openai_client,\n",
    "                              db_name: str = \"document_store\",\n",
    "                              collection_name: str = \"pdf_chunks\",\n",
    "                              top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Complete RAG pipeline: search documents and answer question\"\"\"\n",
    "    print(f\"\\nüîç QUESTION: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Step 1: Perform vector search\n",
    "    print(\"Searching for relevant documents...\")\n",
    "    search_results = vector_search_query(\n",
    "        query_text=question,\n",
    "        mongodb_client=mongodb_client,\n",
    "        openai_client=openai_client,\n",
    "        db_name=db_name,\n",
    "        collection_name=collection_name,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(search_results)} relevant chunks\")\n",
    "    \n",
    "    # Step 2: Generate answer using GPT-4o-mini\n",
    "    print(\"Generating answer...\")\n",
    "    answer = answer_question_with_context(question, search_results, openai_client)\n",
    "    \n",
    "    # Step 3: Display results\n",
    "    print(f\"\\nüí° ANSWER:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(answer)\n",
    "    \n",
    "    if search_results:\n",
    "        print(f\"\\nüìö SOURCES:\")\n",
    "        print(\"-\" * 40)\n",
    "        for i, result in enumerate(search_results[:3], 1):  # Show top 3 sources\n",
    "            score = result.get('score', 0)\n",
    "            chunk_id = result.get('chunk_id', 'Unknown')\n",
    "            source_file = result.get('source_file', 'Unknown')\n",
    "            print(f\"{i}. {source_file} (Chunk {chunk_id}) - Similarity: {score:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": search_results,\n",
    "        \"num_sources\": len(search_results)\n",
    "    }\n",
    "\n",
    "def batch_question_answering(questions: List[str], mongodb_client, openai_client,\n",
    "                           top_k: int = 5, show_detailed_results: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process multiple questions and return all results\"\"\"\n",
    "    print(\"ü§ñ STARTING BATCH QUESTION ANSWERING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nüìù PROCESSING QUESTION {i}/{len(questions)}\")\n",
    "        result = search_and_answer_question(\n",
    "            question=question,\n",
    "            mongodb_client=mongodb_client,\n",
    "            openai_client=openai_client,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Show detailed search results for first question if requested\n",
    "        if i == 1 and show_detailed_results and result['sources']:\n",
    "            print(f\"\\nüìã DETAILED CONTEXT FOR FIRST QUESTION:\")\n",
    "            format_search_results(result['sources'][:2], question)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Updated list of questions instead of simple queries\n",
    "test_questions = [\n",
    "    \"What is the name of the company?\",\n",
    "    \"Who is the CEO of the company?\",\n",
    "    \"What is their vacation policy?\",\n",
    "    \"What is the termination policy?\",\n",
    "]\n",
    "\n",
    "print(\"üöÄ TESTING QUESTION ANSWERING WITH RAG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process all questions\n",
    "results = batch_question_answering(\n",
    "    questions=test_questions,\n",
    "    mongodb_client=mongo_client,\n",
    "    openai_client=openai_client,\n",
    "    top_k=5,\n",
    "    show_detailed_results=True\n",
    ")\n",
    "\n",
    "# Summary of all results\n",
    "print(\"\\nüìä SUMMARY OF ALL QUESTIONS AND ANSWERS\")\n",
    "print(\"=\" * 80)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Q: {result['question']}\")\n",
    "    print(f\"   A: {result['answer'][:100]}...\" if len(result['answer']) > 100 else f\"   A: {result['answer']}\")\n",
    "    print(f\"   Sources: {result['num_sources']} relevant chunks found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "206edb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Local RAG Pipeline...\n",
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "‚úÖ Pipeline initialized with embedding dimension: 384\n",
      "üîÑ Processing PDF...\n",
      "üìÑ Extracting text from: handbook.pdf\n",
      "‚úÖ Extracted 137452 characters\n",
      "‚úÇÔ∏è Creating text chunks...\n",
      "‚úÖ Created 62 chunks\n",
      "üßÆ Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df337987ce45482fb15eeb3a681fc162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Building FAISS index...\n",
      "‚úÖ Index built with 62 vectors\n",
      "üíæ Saved all files to: /Users/thyag/Desktop/Assignement/assignment-zania/dataset/rag_output\n",
      "ü§ñ STARTING QUESTION ANSWERING\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 1/4\n",
      "\n",
      "üîç QUESTION: What is the name of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "The name of the company is Zania, Inc.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. Chunk 4 - Similarity: 0.2486\n",
      "2. Chunk 3 - Similarity: 0.2103\n",
      "3. Chunk 60 - Similarity: 0.1820\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 2/4\n",
      "\n",
      "üîç QUESTION: Who is the CEO of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "The CEO of the company is Shruti Gupta.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. Chunk 3 - Similarity: 0.2717\n",
      "2. Chunk 4 - Similarity: 0.2085\n",
      "3. Chunk 60 - Similarity: 0.1791\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 3/4\n",
      "\n",
      "üîç QUESTION: What is their vacation policy?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "Zania, Inc. provides paid vacation to all full-time regular employees, who are eligible to begin using vacation immediately upon hire or after completing an introductory period. Employees must request vacation from their Manager in advance, considering business needs, and priority may be based on length of employment or seniority if multiple requests are made for the same time off. Vacation must be taken in increments of at least a specified number of hours or days.\n",
      "\n",
      "Unused vacation may be carried over to the following year or forfeited, depending on the company's policy. Upon separation of employment, employees will either forfeit or be paid for any earned but unused vacation time, unless state law dictates otherwise. \n",
      "\n",
      "Vacation is calculated based on the employee's length of service, either granted in a lump sum at the beginning of the year or accrued over time, with specific amounts designated for different years of employment.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. Chunk 33 - Similarity: 0.5017\n",
      "2. Chunk 32 - Similarity: 0.4460\n",
      "3. Chunk 27 - Similarity: 0.3943\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù PROCESSING QUESTION 4/4\n",
      "\n",
      "üîç QUESTION: What is the termination policy?\n",
      "--------------------------------------------------------------------------------\n",
      "Searching for relevant documents...\n",
      "Found 5 relevant chunks\n",
      "Generating answer...\n",
      "\n",
      "üí° ANSWER:\n",
      "================================================================================\n",
      "The termination policy at Zania, Inc. is based on \"at-will\" employment, meaning that either the employee or the Company can terminate employment at any time, with or without notice and with or without cause. The Company is not required to engage in progressive discipline and may discipline or terminate employees for violations of conduct rules or if their work quality does not meet expectations at any time. While management may provide verbal and written warnings before termination, they are not obligated to follow any specific disciplinary or grievance procedure, and employees may be terminated without prior warning.\n",
      "\n",
      "üìö SOURCES:\n",
      "----------------------------------------\n",
      "1. Chunk 13 - Similarity: 0.5439\n",
      "2. Chunk 3 - Similarity: 0.4537\n",
      "3. Chunk 2 - Similarity: 0.4361\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY OF ALL QUESTIONS AND ANSWERS\n",
      "================================================================================\n",
      "\n",
      "1. Q: What is the name of the company?\n",
      "   A: The name of the company is Zania, Inc.\n",
      "   Sources: 5 relevant chunks found\n",
      "\n",
      "2. Q: Who is the CEO of the company?\n",
      "   A: The CEO of the company is Shruti Gupta.\n",
      "   Sources: 5 relevant chunks found\n",
      "\n",
      "3. Q: What is their vacation policy?\n",
      "   A: Zania, Inc. provides paid vacation to all full-time regular employees, who are eligible to begin usi...\n",
      "   Sources: 5 relevant chunks found\n",
      "\n",
      "4. Q: What is the termination policy?\n",
      "   A: The termination policy at Zania, Inc. is based on \"at-will\" employment, meaning that either the empl...\n",
      "   Sources: 5 relevant chunks found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import PyPDF2\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI()\n",
    "\n",
    "class LocalRAGPipeline:\n",
    "    def __init__(self, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the RAG pipeline with local components\"\"\"\n",
    "        print(\"üöÄ Initializing Local RAG Pipeline...\")\n",
    "        \n",
    "        # Load HuggingFace embedding model\n",
    "        print(f\"Loading embedding model: {embedding_model_name}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Initialize FAISS index for vector search\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        self.chunk_metadata = []\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline initialized with embedding dimension: {self.embedding_dim}\")\n",
    "    \n",
    "    def extract_pdf_to_text(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        print(f\"üìÑ Extracting text from: {os.path.basename(pdf_path)}\")\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        print(f\"‚úÖ Extracted {len(text)} characters\")\n",
    "        return text\n",
    "    \n",
    "    def text_to_markdown(self, text: str) -> str:\n",
    "        \"\"\"Convert plain text to basic markdown format\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        markdown_text = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if len(line) < 100 and line.isupper():\n",
    "                    markdown_text += f\"# {line}\\n\\n\"\n",
    "                elif line.endswith(':') and len(line) < 80:\n",
    "                    markdown_text += f\"## {line}\\n\\n\"\n",
    "                else:\n",
    "                    markdown_text += f\"{line}\\n\\n\"\n",
    "        \n",
    "        return markdown_text\n",
    "    \n",
    "    def count_tokens(self, text: str, model: str = \"gpt-4o-mini\") -> int:\n",
    "        \"\"\"Count tokens in text using tiktoken\"\"\"\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    def create_overlapping_chunks(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "        \"\"\"Create overlapping text chunks with specified token sizes\"\"\"\n",
    "        print(\"‚úÇÔ∏è Creating text chunks...\")\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "        tokens = encoding.encode(text)\n",
    "        chunks = []\n",
    "        \n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(tokens):\n",
    "            end = min(start + chunk_size, len(tokens))\n",
    "            chunk_tokens = tokens[start:end]\n",
    "            chunk_text = encoding.decode(chunk_tokens)\n",
    "            \n",
    "            chunk_data = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_text,\n",
    "                \"token_count\": len(chunk_tokens)\n",
    "            }\n",
    "            \n",
    "            chunks.append(chunk_data)\n",
    "            start = end - overlap\n",
    "            chunk_id += 1\n",
    "            \n",
    "            if end >= len(tokens):\n",
    "                break\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings using HuggingFace model\"\"\"\n",
    "        print(\"üßÆ Generating embeddings...\")\n",
    "        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def build_index(self, embeddings: np.ndarray) -> None:\n",
    "        \"\"\"Build FAISS index for vector search\"\"\"\n",
    "        print(\"üîç Building FAISS index...\")\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product for cosine similarity\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        print(f\"‚úÖ Index built with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str, save_dir: str = None) -> str:\n",
    "        \"\"\"Complete pipeline to process PDF and create local index\"\"\"\n",
    "        if save_dir is None:\n",
    "            save_dir = os.path.dirname(pdf_path)\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Step 1: Extract and convert text\n",
    "        raw_text = self.extract_pdf_to_text(pdf_path)\n",
    "        markdown_text = self.text_to_markdown(raw_text)\n",
    "        \n",
    "        # Step 2: Create chunks\n",
    "        chunks = self.create_overlapping_chunks(markdown_text)\n",
    "        self.chunks = [chunk['text'] for chunk in chunks]\n",
    "        self.chunk_metadata = chunks\n",
    "        \n",
    "        # Step 3: Generate embeddings\n",
    "        embeddings = self.generate_embeddings(self.chunks)\n",
    "        \n",
    "        # Step 4: Build index\n",
    "        self.build_index(embeddings)\n",
    "        \n",
    "        # Step 5: Save everything locally\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        \n",
    "        # Save chunks and metadata\n",
    "        chunks_file = os.path.join(save_dir, f\"{base_name}_chunks.pkl\")\n",
    "        with open(chunks_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'chunks': self.chunks,\n",
    "                'metadata': self.chunk_metadata,\n",
    "                'embeddings': embeddings\n",
    "            }, f)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        index_file = os.path.join(save_dir, f\"{base_name}_index.faiss\")\n",
    "        faiss.write_index(self.index, index_file)\n",
    "        \n",
    "        # Save markdown\n",
    "        markdown_file = os.path.join(save_dir, f\"{base_name}.md\")\n",
    "        with open(markdown_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_text)\n",
    "        \n",
    "        print(f\"üíæ Saved all files to: {save_dir}\")\n",
    "        return save_dir\n",
    "    \n",
    "    def load_index(self, save_dir: str, base_name: str) -> None:\n",
    "        \"\"\"Load previously saved index and chunks\"\"\"\n",
    "        print(f\"üìÇ Loading saved index from: {save_dir}\")\n",
    "        \n",
    "        # Load chunks and metadata\n",
    "        chunks_file = os.path.join(save_dir, f\"{base_name}_chunks.pkl\")\n",
    "        with open(chunks_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.chunks = data['chunks']\n",
    "            self.chunk_metadata = data['metadata']\n",
    "        \n",
    "        # Load FAISS index\n",
    "        index_file = os.path.join(save_dir, f\"{base_name}_index.faiss\")\n",
    "        self.index = faiss.read_index(index_file)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.chunks)} chunks and index with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def search_similar_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar chunks using vector similarity\"\"\"\n",
    "        if self.index is None:\n",
    "            print(\"‚ùå No index available. Please process a PDF first.\")\n",
    "            return []\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search in index\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx != -1:  # Valid result\n",
    "                result = {\n",
    "                    'chunk_id': idx,\n",
    "                    'text': self.chunks[idx],\n",
    "                    'score': float(score),\n",
    "                    'metadata': self.chunk_metadata[idx]\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer_question_with_context(self, question: str, context_chunks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Use GPT-4o-mini to answer questions based on retrieved context\"\"\"\n",
    "        if not context_chunks:\n",
    "            return \"I don't have enough information to answer this question.\"\n",
    "        \n",
    "        # Combine context from all chunks\n",
    "        context_text = \"\\n\\n\".join([chunk.get('text', '') for chunk in context_chunks])\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following context from the document, please answer the question. If the answer is not clearly available in the context, please say so.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided document context. Be concise and accurate.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=500,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            return \"Sorry, I couldn't generate an answer due to an error.\"\n",
    "    \n",
    "    def search_and_answer_question(self, question: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Complete RAG pipeline: search documents and answer question\"\"\"\n",
    "        print(f\"\\nüîç QUESTION: {question}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Step 1: Search for relevant chunks\n",
    "        print(\"Searching for relevant documents...\")\n",
    "        search_results = self.search_similar_chunks(question, top_k)\n",
    "        print(f\"Found {len(search_results)} relevant chunks\")\n",
    "        \n",
    "        # Step 2: Generate answer\n",
    "        print(\"Generating answer...\")\n",
    "        answer = self.answer_question_with_context(question, search_results)\n",
    "        \n",
    "        # Step 3: Display results\n",
    "        print(f\"\\nüí° ANSWER:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(answer)\n",
    "        \n",
    "        if search_results:\n",
    "            print(f\"\\nüìö SOURCES:\")\n",
    "            print(\"-\" * 40)\n",
    "            for i, result in enumerate(search_results[:3], 1):\n",
    "                score = result.get('score', 0)\n",
    "                chunk_id = result.get('chunk_id', 'Unknown')\n",
    "                print(f\"{i}. Chunk {chunk_id} - Similarity: {score:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": search_results,\n",
    "            \"num_sources\": len(search_results)\n",
    "        }\n",
    "    \n",
    "    def process_questions(self, questions: List[str], top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple questions and return all results\"\"\"\n",
    "        print(\"ü§ñ STARTING QUESTION ANSWERING\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\nüìù PROCESSING QUESTION {i}/{len(questions)}\")\n",
    "            result = self.search_and_answer_question(question, top_k)\n",
    "            all_results.append(result)\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "def run_rag_pipeline(pdf_path: str, questions: List[str], \n",
    "                    save_dir: str = None, force_reprocess: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Main function to run the complete RAG pipeline\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        questions: List of questions to answer\n",
    "        save_dir: Directory to save processed files (optional)\n",
    "        force_reprocess: Force reprocessing even if files exist\n",
    "    \n",
    "    Returns:\n",
    "        List of question-answer results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    rag = LocalRAGPipeline()\n",
    "    \n",
    "    # Set up paths\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(os.path.dirname(pdf_path), \"rag_output\")\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    chunks_file = os.path.join(save_dir, f\"{base_name}_chunks.pkl\")\n",
    "    \n",
    "    # Check if we need to process or can load existing\n",
    "    if not force_reprocess and os.path.exists(chunks_file):\n",
    "        print(\"üìÇ Found existing processed files, loading...\")\n",
    "        rag.load_index(save_dir, base_name)\n",
    "    else:\n",
    "        print(\"üîÑ Processing PDF...\")\n",
    "        rag.process_pdf(pdf_path, save_dir)\n",
    "    \n",
    "    # Answer questions\n",
    "    results = rag.process_questions(questions)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nüìä SUMMARY OF ALL QUESTIONS AND ANSWERS\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Q: {result['question']}\")\n",
    "        answer_preview = result['answer'][:100] + \"...\" if len(result['answer']) > 100 else result['answer']\n",
    "        print(f\"   A: {answer_preview}\")\n",
    "        print(f\"   Sources: {result['num_sources']} relevant chunks found\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define inputs\n",
    "    pdf_file_path = \"/Users/thyag/Desktop/Assignement/assignment-zania/dataset/raw-data/handbook.pdf\"\n",
    "    \n",
    "    test_questions = [\n",
    "        \"What is the name of the company?\",\n",
    "        \"Who is the CEO of the company?\",\n",
    "        \"What is their vacation policy?\",\n",
    "        \"What is the termination policy?\",\n",
    "    ]\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    results = run_rag_pipeline(\n",
    "        pdf_path=pdf_file_path,\n",
    "        questions=test_questions,\n",
    "        save_dir=\"/Users/thyag/Desktop/Assignement/assignment-zania/dataset/rag_output\",\n",
    "        force_reprocess=False \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84bbaedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:03:55,350 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: mps\n",
      "2025-06-25 23:03:55,350 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-06-25 23:03:59,329 - __main__ - INFO - Loaded embedding model: all-MiniLM-L6-v2 (dim: 384)\n",
      "2025-06-25 23:03:59,355 - __main__ - INFO - Initialized ProductionRAGPipeline\n",
      "2025-06-25 23:03:59,357 - __main__ - INFO - Loading existing processed data...\n",
      "2025-06-25 23:03:59,375 - __main__ - INFO - Loaded FAISS index from /Users/thyag/Desktop/Assignement/assignment-zania/dataset/rag_output/handbook_index.faiss\n",
      "2025-06-25 23:03:59,376 - __main__ - INFO - Loaded 62 chunks and index from /Users/thyag/Desktop/Assignement/assignment-zania/dataset/rag_output\n",
      "2025-06-25 23:03:59,376 - __main__ - INFO - Processing question 1/4: What is the name of the company?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a570a2f98d4fb49492d8cb30d11868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:03:59,604 - __main__ - INFO - Generated embeddings for 1 texts\n",
      "2025-06-25 23:04:01,120 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 23:04:01,129 - __main__ - INFO - Processed question: What is the name of the company?\n",
      "2025-06-25 23:04:01,130 - __main__ - INFO - Processing question 2/4: Who is the CEO of the company?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56f048d367b47d288e6fe3071c82781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:04:01,252 - __main__ - INFO - Generated embeddings for 1 texts\n",
      "2025-06-25 23:04:02,658 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 23:04:02,659 - __main__ - INFO - Processed question: Who is the CEO of the company?\n",
      "2025-06-25 23:04:02,660 - __main__ - INFO - Processing question 3/4: What is their vacation policy?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98acf9dc559d440194ddfa805aac0fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:04:02,802 - __main__ - INFO - Generated embeddings for 1 texts\n",
      "2025-06-25 23:04:05,569 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 23:04:05,573 - __main__ - INFO - Processed question: What is their vacation policy?\n",
      "2025-06-25 23:04:05,574 - __main__ - INFO - Processing question 4/4: What is the termination policy?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1c45aabaeb4b3c86536d406c8e1e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:04:05,656 - __main__ - INFO - Generated embeddings for 1 texts\n",
      "2025-06-25 23:04:11,892 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 23:04:11,895 - __main__ - INFO - Processed question: What is the termination policy?\n",
      "2025-06-25 23:04:11,898 - __main__ - INFO - Saved results to /Users/thyag/Desktop/Assignement/assignment-zania/dataset/rag_output/qa_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import tiktoken\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('rag_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration class for RAG pipeline\"\"\"\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    chunk_size: int = 500\n",
    "    chunk_overlap: int = 50\n",
    "    max_tokens_openai: int = 500\n",
    "    temperature: float = 0.1\n",
    "    top_k: int = 5\n",
    "    openai_model: str = \"gpt-4o-mini\"\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'embedding_model': self.embedding_model,\n",
    "            'chunk_size': self.chunk_size,\n",
    "            'chunk_overlap': self.chunk_overlap,\n",
    "            'max_tokens_openai': self.max_tokens_openai,\n",
    "            'temperature': self.temperature,\n",
    "            'top_k': self.top_k,\n",
    "            'openai_model': self.openai_model\n",
    "        }\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Handles PDF text extraction and preprocessing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text(pdf_path: Path) -> str:\n",
    "        \"\"\"Extract text from PDF file with error handling\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    try:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error extracting text from page {page_num}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if not text.strip():\n",
    "                    raise ValueError(\"No text extracted from PDF\")\n",
    "                \n",
    "                logger.info(f\"Extracted {len(text)} characters from {pdf_path.name}\")\n",
    "                return text\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to extract text from {pdf_path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def text_to_markdown(text: str) -> str:\n",
    "        \"\"\"Convert plain text to markdown with improved formatting\"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        markdown_text = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Header detection with better heuristics\n",
    "            if len(line) < 100 and line.isupper() and len(line.split()) > 1:\n",
    "                markdown_text += f\"# {line}\\n\\n\"\n",
    "            elif line.endswith(':') and len(line) < 80 and not line.count('.') > 2:\n",
    "                markdown_text += f\"## {line}\\n\\n\"\n",
    "            elif line.startswith(('‚Ä¢', '-', '*')) or line.lstrip().startswith(tuple('123456789')):\n",
    "                markdown_text += f\"{line}\\n\\n\"\n",
    "            else:\n",
    "                markdown_text += f\"{line}\\n\\n\"\n",
    "        \n",
    "        return markdown_text\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"Handles text chunking with overlap\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.encoding = tiktoken.encoding_for_model(model_name)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def create_overlapping_chunks(\n",
    "        self, \n",
    "        text: str, \n",
    "        chunk_size: int = 500, \n",
    "        overlap: int = 50\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create overlapping text chunks with metadata\"\"\"\n",
    "        try:\n",
    "            tokens = self.encoding.encode(text)\n",
    "            chunks = []\n",
    "            \n",
    "            if len(tokens) <= chunk_size:\n",
    "                # Single chunk if text is small enough\n",
    "                return [{\n",
    "                    \"chunk_id\": 0,\n",
    "                    \"text\": text,\n",
    "                    \"token_count\": len(tokens),\n",
    "                    \"start_token\": 0,\n",
    "                    \"end_token\": len(tokens)\n",
    "                }]\n",
    "            \n",
    "            start = 0\n",
    "            chunk_id = 0\n",
    "            \n",
    "            while start < len(tokens):\n",
    "                end = min(start + chunk_size, len(tokens))\n",
    "                chunk_tokens = tokens[start:end]\n",
    "                chunk_text = self.encoding.decode(chunk_tokens)\n",
    "                \n",
    "                chunk_data = {\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"token_count\": len(chunk_tokens),\n",
    "                    \"start_token\": start,\n",
    "                    \"end_token\": end\n",
    "                }\n",
    "                \n",
    "                chunks.append(chunk_data)\n",
    "                \n",
    "                if end >= len(tokens):\n",
    "                    break\n",
    "                    \n",
    "                start = end - overlap\n",
    "                chunk_id += 1\n",
    "            \n",
    "            logger.info(f\"Created {len(chunks)} chunks\")\n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating chunks: {e}\")\n",
    "            raise\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Handles embedding generation and caching\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "            logger.info(f\"Loaded embedding model: {model_name} (dim: {self.embedding_dim})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load embedding model {model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings with batching\"\"\"\n",
    "        try:\n",
    "            if not texts:\n",
    "                raise ValueError(\"No texts provided for embedding generation\")\n",
    "            \n",
    "            embeddings = self.model.encode(\n",
    "                texts, \n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Generated embeddings for {len(texts)} texts\")\n",
    "            return embeddings\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Handles vector storage and retrieval using FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = None\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def build_index(self, embeddings: np.ndarray, use_gpu: bool = False) -> None:\n",
    "        \"\"\"Build FAISS index with optional GPU support\"\"\"\n",
    "        try:\n",
    "            if embeddings.shape[1] != self.embedding_dim:\n",
    "                raise ValueError(f\"Embedding dimension mismatch: {embeddings.shape[1]} != {self.embedding_dim}\")\n",
    "            \n",
    "            # Use IndexFlatIP for cosine similarity\n",
    "            self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "            \n",
    "            # Normalize embeddings for cosine similarity\n",
    "            embeddings_normalized = embeddings.copy().astype('float32')\n",
    "            faiss.normalize_L2(embeddings_normalized)\n",
    "            \n",
    "            # Add to GPU if requested and available\n",
    "            if use_gpu and faiss.get_num_gpus() > 0:\n",
    "                gpu_index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, self.index)\n",
    "                gpu_index.add(embeddings_normalized)\n",
    "                self.index = gpu_index\n",
    "                logger.info(\"Using GPU for FAISS index\")\n",
    "            else:\n",
    "                self.index.add(embeddings_normalized)\n",
    "            \n",
    "            self.is_trained = True\n",
    "            logger.info(f\"Built FAISS index with {self.index.ntotal} vectors\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building FAISS index: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Search for similar vectors\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        try:\n",
    "            # Normalize query embedding\n",
    "            query_normalized = query_embedding.copy().astype('float32')\n",
    "            faiss.normalize_L2(query_normalized)\n",
    "            \n",
    "            scores, indices = self.index.search(query_normalized, top_k)\n",
    "            return scores, indices\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching index: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save(self, filepath: Path) -> None:\n",
    "        \"\"\"Save FAISS index to disk\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError(\"No index to save\")\n",
    "        \n",
    "        try:\n",
    "            faiss.write_index(self.index, str(filepath))\n",
    "            logger.info(f\"Saved FAISS index to {filepath}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving FAISS index: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load(self, filepath: Path) -> None:\n",
    "        \"\"\"Load FAISS index from disk\"\"\"\n",
    "        try:\n",
    "            self.index = faiss.read_index(str(filepath))\n",
    "            self.is_trained = True\n",
    "            logger.info(f\"Loaded FAISS index from {filepath}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading FAISS index: {e}\")\n",
    "            raise\n",
    "\n",
    "class OpenAIClient:\n",
    "    \"\"\"Wrapper for OpenAI API with error handling and retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def generate_answer(\n",
    "        self, \n",
    "        question: str, \n",
    "        context: str, \n",
    "        model: str = \"gpt-4o-mini\",\n",
    "        max_tokens: int = 500,\n",
    "        temperature: float = 0.1\n",
    "    ) -> str:\n",
    "        \"\"\"Generate answer using OpenAI API with retry logic\"\"\"\n",
    "        prompt = f\"\"\"Based on the following context from the document, please answer the question. If the answer is not clearly available in the context, please say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided document context. Be concise and accurate. Do not format the answer and return plain text.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer with OpenAI: {e}\")\n",
    "            return \"Sorry, I couldn't generate an answer due to an error.\"\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"RAG pipeline with comprehensive error handling and logging\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, openai_api_key: Optional[str] = None):\n",
    "        self.config = config\n",
    "        self.pdf_processor = PDFProcessor()\n",
    "        self.text_chunker = TextChunker(config.openai_model)\n",
    "        self.embedding_generator = EmbeddingGenerator(config.embedding_model)\n",
    "        self.vector_store = VectorStore(self.embedding_generator.embedding_dim)\n",
    "        self.openai_client = OpenAIClient(openai_api_key)\n",
    "        \n",
    "        # Data storage\n",
    "        self.chunks = []\n",
    "        self.chunk_metadata = []\n",
    "        self.processed_files = {}\n",
    "        \n",
    "        logger.info(\"Initialized ProductionRAGPipeline\")\n",
    "    \n",
    "    def process_pdf(self, pdf_path: Path, save_dir: Optional[Path] = None) -> Path:\n",
    "        \"\"\"Process PDF and create embeddings with comprehensive error handling\"\"\"\n",
    "        try:\n",
    "            pdf_path = Path(pdf_path)\n",
    "            if not pdf_path.exists():\n",
    "                raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "            \n",
    "            if save_dir is None:\n",
    "                save_dir = pdf_path.parent / \"rag_output\"\n",
    "            \n",
    "            save_dir = Path(save_dir)\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            base_name = pdf_path.stem\n",
    "            \n",
    "            logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "            \n",
    "            # Step 1: Extract and convert text\n",
    "            raw_text = self.pdf_processor.extract_text(pdf_path)\n",
    "            markdown_text = self.pdf_processor.text_to_markdown(raw_text)\n",
    "            \n",
    "            # Step 2: Create chunks\n",
    "            chunks = self.text_chunker.create_overlapping_chunks(\n",
    "                markdown_text, \n",
    "                self.config.chunk_size, \n",
    "                self.config.chunk_overlap\n",
    "            )\n",
    "            \n",
    "            self.chunks = [chunk['text'] for chunk in chunks]\n",
    "            self.chunk_metadata = chunks\n",
    "            \n",
    "            # Step 3: Generate embeddings\n",
    "            embeddings = self.embedding_generator.generate_embeddings(self.chunks)\n",
    "            \n",
    "            # Step 4: Build vector index\n",
    "            self.vector_store.build_index(embeddings)\n",
    "            \n",
    "            # Step 5: Save everything\n",
    "            self._save_processed_data(save_dir, base_name, markdown_text, embeddings)\n",
    "            \n",
    "            # Update processed files record\n",
    "            self.processed_files[str(pdf_path)] = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'save_dir': str(save_dir),\n",
    "                'base_name': base_name,\n",
    "                'num_chunks': len(chunks),\n",
    "                'config': self.config.to_dict()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Successfully processed PDF: {pdf_path}\")\n",
    "            return save_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _save_processed_data(\n",
    "        self, \n",
    "        save_dir: Path, \n",
    "        base_name: str, \n",
    "        markdown_text: str, \n",
    "        embeddings: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"Save all processed data to disk\"\"\"\n",
    "        try:\n",
    "            # Save chunks and metadata\n",
    "            chunks_file = save_dir / f\"{base_name}_chunks.pkl\"\n",
    "            with open(chunks_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'chunks': self.chunks,\n",
    "                    'metadata': self.chunk_metadata,\n",
    "                    'embeddings': embeddings,\n",
    "                    'config': self.config.to_dict(),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }, f)\n",
    "            \n",
    "            # Save FAISS index\n",
    "            index_file = save_dir / f\"{base_name}_index.faiss\"\n",
    "            self.vector_store.save(index_file)\n",
    "            \n",
    "            # Save markdown\n",
    "            markdown_file = save_dir / f\"{base_name}.md\"\n",
    "            with open(markdown_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(markdown_text)\n",
    "            \n",
    "            # Save processing metadata\n",
    "            metadata_file = save_dir / f\"{base_name}_metadata.json\"\n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'num_chunks': len(self.chunks),\n",
    "                    'embedding_dim': self.embedding_generator.embedding_dim,\n",
    "                    'config': self.config.to_dict(),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            logger.info(f\"Saved all processed data to: {save_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving processed data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_processed_data(self, save_dir: Path, base_name: str) -> None:\n",
    "        \"\"\"Load previously processed data\"\"\"\n",
    "        try:\n",
    "            save_dir = Path(save_dir)\n",
    "            \n",
    "            # Load chunks and metadata\n",
    "            chunks_file = save_dir / f\"{base_name}_chunks.pkl\"\n",
    "            if not chunks_file.exists():\n",
    "                raise FileNotFoundError(f\"Chunks file not found: {chunks_file}\")\n",
    "            \n",
    "            with open(chunks_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                self.chunks = data['chunks']\n",
    "                self.chunk_metadata = data['metadata']\n",
    "            \n",
    "            # Load FAISS index\n",
    "            index_file = save_dir / f\"{base_name}_index.faiss\"\n",
    "            if not index_file.exists():\n",
    "                raise FileNotFoundError(f\"Index file not found: {index_file}\")\n",
    "            \n",
    "            self.vector_store.load(index_file)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(self.chunks)} chunks and index from {save_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading processed data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def search_and_answer(self, question: str, top_k: Optional[int] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Search for relevant chunks and generate answer\"\"\"\n",
    "        try:\n",
    "            if top_k is None:\n",
    "                top_k = self.config.top_k\n",
    "            \n",
    "            if not self.vector_store.is_trained:\n",
    "                raise RuntimeError(\"Vector store not ready. Process a PDF first.\")\n",
    "            \n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_generator.generate_embeddings([question])\n",
    "            \n",
    "            # Search for similar chunks\n",
    "            scores, indices = self.vector_store.search(query_embedding, top_k)\n",
    "            \n",
    "            # Prepare search results\n",
    "            search_results = []\n",
    "            for score, idx in zip(scores[0], indices[0]):\n",
    "                if idx != -1:  # Valid result\n",
    "                    result = {\n",
    "                        'chunk_id': int(idx),\n",
    "                        'text': self.chunks[idx],\n",
    "                        'score': float(score),\n",
    "                        'metadata': self.chunk_metadata[idx]\n",
    "                    }\n",
    "                    search_results.append(result)\n",
    "            \n",
    "            # Generate answer\n",
    "            if search_results:\n",
    "                context_text = \"\\n\\n\".join([chunk['text'] for chunk in search_results])\n",
    "                answer = self.openai_client.generate_answer(\n",
    "                    question, \n",
    "                    context_text,\n",
    "                    self.config.openai_model,\n",
    "                    self.config.max_tokens_openai,\n",
    "                    self.config.temperature\n",
    "                )\n",
    "            else:\n",
    "                answer = \"I don't have enough information to answer this question.\"\n",
    "            \n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"sources\": search_results,\n",
    "                \"num_sources\": len(search_results),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Processed question: {question}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing question '{question}': {e}\")\n",
    "            raise\n",
    "    \n",
    "    def batch_process_questions(self, questions: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple questions in batch\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            try:\n",
    "                logger.info(f\"Processing question {i}/{len(questions)}: {question}\")\n",
    "                result = self.search_and_answer(question)\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing question {i}: {e}\")\n",
    "                error_result = {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": f\"Error processing question: {str(e)}\",\n",
    "                    \"sources\": [],\n",
    "                    \"num_sources\": 0,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"error\": True\n",
    "                }\n",
    "                results.append(error_result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def create_rag_pipeline(\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "    openai_api_key: Optional[str] = None,\n",
    "    **config_kwargs\n",
    ") -> RAGPipeline:\n",
    "    \"\"\"Factory function to create RAG pipeline\"\"\"\n",
    "    config = RAGConfig(embedding_model=embedding_model, **config_kwargs)\n",
    "    return RAGPipeline(config, openai_api_key)\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    import dotenv\n",
    "    \n",
    "    # Load environment variables\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    # Create pipeline\n",
    "    rag = create_rag_pipeline(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    # Process PDF\n",
    "    pdf_path = Path(\"/Users/thyag/Desktop/Assignement/assignment-zania/dataset/raw-data/handbook.pdf\")\n",
    "    save_dir = Path(\"/Users/thyag/Desktop/Assignement/assignment-zania/dataset/rag_output\")\n",
    "    \n",
    "    try:\n",
    "        # Check if already processed\n",
    "        base_name = pdf_path.stem\n",
    "        chunks_file = save_dir / f\"{base_name}_chunks.pkl\"\n",
    "        \n",
    "        if chunks_file.exists():\n",
    "            logger.info(\"Loading existing processed data...\")\n",
    "            rag.load_processed_data(save_dir, base_name)\n",
    "        else:\n",
    "            logger.info(\"Processing PDF...\")\n",
    "            rag.process_pdf(pdf_path, save_dir)\n",
    "        \n",
    "        # Process questions\n",
    "        questions = [\n",
    "            \"What is the name of the company?\",\n",
    "            \"Who is the CEO of the company?\",\n",
    "            \"What is their vacation policy?\",\n",
    "            \"What is the termination policy?\",\n",
    "        ]\n",
    "        \n",
    "        results = rag.batch_process_questions(questions)\n",
    "        \n",
    "        # Save results\n",
    "        results_file = save_dir / \"qa_results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved results to {results_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794d226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
